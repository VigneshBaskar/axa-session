{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Scripts.CreateTrainingBatches import CreateTrainingBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('Data','data_X_y.p'), 'rb') as handle:\n",
    "    data_X_y = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join('Data','training_params.p'), 'rb') as handle:\n",
    "    training_params = pickle.load(handle)\n",
    "    \n",
    "rev_vocab_dict = training_params['rev_vocab_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = [data_X_y['X_train'],data_X_y['X_valid'], data_X_y['X_test']]\n",
    "y_train, y_valid, y_test = [data_X_y['y_train'],data_X_y['y_valid'], data_X_y['y_test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_actual_text(x, rev_vocab_dict):\n",
    "    actual_text = \" \".join([rev_vocab_dict[word_id] for word_id in x  if rev_vocab_dict[word_id]!='my_dummy'])\n",
    "    return actual_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_num = 1\n",
    "return_actual_text(X_train[doc_num], rev_vocab_dict), y_train[doc_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = training_params['vocab_size']\n",
    "mlb = MultiLabelBinarizer()\n",
    "X_train = mlb.fit_transform(X_train)\n",
    "X_valid = mlb.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_training_batches_object = CreateTrainingBatches(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_metrics(np_prob, np_y):\n",
    "    neg_accuracy = np.mean((np_prob<0.5)[(np_y==0)])\n",
    "    pos_accuracy = np.mean((np_prob>0.5)[(np_y==1)])\n",
    "    accuracy = np.mean((pos_accuracy, neg_accuracy))\n",
    "    print('Negative accuracy',neg_accuracy)\n",
    "    print('Positive accuracy',pos_accuracy)\n",
    "    print('Accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an input $x$, the model should be able to map it to $\\hat{y}$ $$\n",
    "\\begin{align}\n",
    "\\hat{y}  = \\sigma(w^Tx+b) \\\n",
    "\\end{align}$$ The paramters of the model are the weights $w$ and the bias $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32,shape=[None, vocab_size], name='X')\n",
    "weights = tf.Variable(tf.random_uniform([X.get_shape().as_list()[1], 1], minval=-1.0, maxval=1.0), name='weights')\n",
    "bias = tf.zeros(1, name='bias')\n",
    "logits = tf.matmul(X, weights) + bias\n",
    "prob = tf.nn.sigmoid(logits, name='prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is an estimate the difference between the true label $y$ and the predicted label $\\hat{y}$ $$\\begin{align}\n",
    "\\ L(\\hat{y},y) = - [y log(\\hat{y}) + (1 - y) log(1 - \\hat{y})] \\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, [None, 1], name='y')\n",
    "losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits, name='x_entropy')\n",
    "loss = tf.reduce_mean(losses, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the Parameter to minimze the loss Here $\\alpha$ is the learning rate and $\\frac{\\partial L}{\\partial w}$ is the partial derivative of Loss $L$ with respect to weight  $w$ $$\\begin{align}\n",
    "\\ w = w - \\alpha \\frac{\\partial L}{\\partial w}\\\\\n",
    "\\ b = b - \\alpha \\frac{\\partial L}{\\partial b}\\\\\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss, name='train_op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter('Data/tf_logs/logistic_regression', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    x_train_samples, y_train_samples = create_training_batches_object.create_training_data()\n",
    "    \n",
    "    _, np_prob, np_y, np_loss = sess.run([training_op, prob, y, loss], \n",
    "                                         feed_dict={X: x_train_samples, y: y_train_samples})\n",
    "    \n",
    "# TODO\n",
    "# Once in every 100 epochs save the best model with highest validation accuracy  \n",
    "# saver.save(sess, os.path.join('Data', 'tf_models','model.ckpt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Use higher level API for feedforward insted of explicit matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Add more layers and repeat the process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axa",
   "language": "python",
   "name": "axa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
